#+OPTIONS: toc:nil num:nil todo:nil
#+LAYOUT: post
#+DATE: 2017-06-18
#+TITLE: Diagnosing Bad Hypothesis Tests
#+DESCRIPTION: I find a strange plot of p-values from a hypothesis test and investigate.
#+CATEGORIES: statistics
#+FEATURED: false

Let's suppose you see the following histogram of p-values: what do you
think is going on?

# Plot histogram of p-values from fitted KS-test
#+BEGIN_SRC R :exports results :results graphics :file img/conservative-p-values.png
  library(ggplot2)
  library(plyr)

  n_samples <- 1000
  n_draw <- 100

  set.seed(42)

  pvalues <- raply(n_samples, function() {
    x <- rnorm(n_draw, 10, 12)
    ks.test(scale(x), pnorm)$p.value
  })

  fig <- ggplot(NULL, aes(x = pvalues)) + geom_histogram() +
    xlab("p-value") + ylab("Count") +
    theme_bw()

  print(fig)
#+END_SRC

#+LABEL: fig:conservative-p-values
#+CAPTION: Histogram of p-values; notice that the p-values are skewed and flipped from the usual clumping around 1 (when the null doesn't hold).

At first I thought there had been some mistake in the code where $p =
1 - p$. But no, it's not a coding mistake.

Alternatively it could be an ill-advised one-sided test. However this
isn't the case since it's two-sided.

My final thought was that some assumption broke. It's always the last
thing you think of.

The test that I was running is the Kolmogorov-Smirnoff (KS) test for
goodness-of-fit. It's designed to test whether a sample came from a
particular distribution by looking at the maximum difference between
the CDF of the sample and the CDF of the distribution. Since I was
working with scaled (mean = 0, sd = 1) data I just compared with the
$N(0,1)$. That's when I saw a figure much like Figure
[[fig:conservative-p-values]] which is actually generated from the null.

This is bad because p-values under the null should be uniform
otherwise we don't have the specified Type-I error. In this case we
have a conservative test so the actual Type-I error is less than than
the specified Type-I error. Usually people are more concerned when a
test is liberal (actual Type-I error greater than specified) since the
whole point of hypothesis testing is to control the Type-I error rate.
However, conservative tests are concerning as well since they leave
power on the table and make more Type-II errors than necessary.

I said before that an assumption broke. It turns out that what's
happening with the scaled is that the CDFs are shifted closer
together. The null distribution for the KS statistic doesn't take this
into account; it assumes your distribution was fixed and not fitted to
the data. Scaling the data implicitly fits the mean and standard
deviation of the normal and thus the assumption is broken.

To emphasize how fragile this assumption makes the KS test I draw
standard normal data and try the KS-test with the scaled data with the
raw data. This rescaling is essentially doing nothing to the data, but
the effects on the test are substantial. Figure [[fig:ks-test-statistics]]
show that the distribution of test statistics for the fitted
distribution (red) is pushed left which will result in low p-values
since the KS test assumes the raw distribution (blue) is the null.

# Plot test statistics under scaled and raw samples from N(0, 1)
#+BEGIN_SRC R :exports results :results graphics :file img/ks-test-statistics.png
  library(directlabels)
  library(ggplot2)
  library(plyr)

  set.seed(42)

  n_samples <- 10000
  n_draw <- 100

  fitted <- raply(n_samples, function() {
    x <- rnorm(n_draw, 0, 1)
    return(ks.test(x, pnorm, mean = mean(x), sd = sd(x))$statistic)
  })

  raw <- raply(n_samples, function() {
    x <- rnorm(n_draw, 0, 1)
    return(ks.test(x, pnorm)$statistic)
  })

  df <- rbind(data.frame(x = fitted,
                         type = "Fitted"),
              data.frame(x = raw,
                         type = "Raw"))

  fig <- ggplot(df, aes(x = x, color = type, fill = type)) + geom_density(alpha = 0.2) +
    xlab("KS Statistic") + ylab("Density") +

    theme_bw()

  print(direct.label(fig))
#+END_SRC

#+LABEL: fig:ks-test-statistics
#+CAPTION: Distributions of KS test statistics for $N(0,1)$ before fitting (blue) and after fitting (red). The effect of fitting the model shifts the distribution of the test statistic to the left, leading to conservative p-values.

With the constraint that you need to specify a single distribution the
KS test becomes less useful. Especially since I wasn't the one doing
the scaling. Fortunately there are workarounds.

For normal distributions there's a modification of the KS test which
will work for the fitted data: the Lillifors test. It uses the same
test statistics just with the correct distribution. I inadvertently
replicated the original paper's [fn:lilliefors1967kolmogorov] method
for determining that distribution in Figure [[fig:ks-test-statistics]];
it's just a Monte Carlo sample from the null. There's also an analytic
approximation [fn:dallal1986analytic] used by R's [[https://cran.r-project.org/web/packages/nortest/index.html][=nortest=]] and
Python's [[https://statsmodels.org][=statsmodels=]].

This idea can be generalized to other location-scale families since
the effect of parameter estimation is the same as a linear
transformation to "canonical" form like the standard normal. There are
also analytic ways of calculating these null
distributions [fn:durbin1973distribution].

Outside of these families the effect of parameter estimation could
depend on the unknown parameters which complicates things greatly.
Fortunately you can use the bootstrap in these cases [fn:babu2004goodness].

* References
[fn:lilliefors1967kolmogorov] Lilliefors. (1967). /On the Kolmogorov-Smirnov test for normality with mean and variance unknown/. JASA. [[https://pdfs.semanticscholar.org/4aad/1756e88dba86399a75891895e00b160f5460.pdf][link]]
# @article{lilliefors1967kolmogorov,
#   title={},
#   author={Lilliefors, Hubert W},
#   journal={Journal of the American Statistical Association},
#   volume={62},
#   number={318},
#   pages={399--402},
#   year={1967},
#   publisher={Taylor \& Francis Group}
# }

[fn:dallal1986analytic] Dallal and Wilkinson. (1986). /An analytic approximation to the distribution of Lilliefors's test statistic for normality/. Taylor & Francis Group. [[http://amstat.tandfonline.com/doi/abs/10.1080/00031305.1986.10475419][link]]
# @article{dallal1986analytic,
#   title={An analytic approximation to the distribution of Lilliefors's test statistic for normality},
#   author={Dallal, Gerard E and Wilkinson, Leland},
#   journal={The American Statistician},
#   volume={40},
#   number={4},
#   pages={294--296},
#   year={1986},
#   publisher={Taylor \& Francis Group}
# }

[fn:durbin1973distribution] Durbin. (1973). /Distribution theory for tests based on the sample distribution function/. SIAM.
# @book{durbin1973distribution,
#   title={Distribution theory for tests based on the sample distribution function},
#   author={Durbin, James},
#   year={1973},
#   publisher={SIAM}
# }

[fn:babu2004goodness] Babu and Rao. (2004). /Goodness-of-fit tests when parameters are estimated/. Sankhy: The Indian Journal of Statistics [[http://sankhya.isical.ac.in/search/66_1/2004005.pdf][link]]
# @article{babu2004goodness,
#   title={Goodness-of-fit tests when parameters are estimated},
#   author={Babu, G Jogesh and Rao, CR},
#   journal={Sankhy{\=a}: The Indian Journal of Statistics},
#   pages={63--74},
#   year={2004},
#   publisher={JSTOR}
# }
