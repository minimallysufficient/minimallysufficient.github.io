---
date: 2017-06-18
tags: 
- 
- 
- 
- 
- 
- dummy-tags-should-be-replaced
author: 
layout: post
title: Diagnosing Bad Hypothesis Tests
excerpt: I find a strange plot of p-values from a hypothesis test and investigate.
categories: 
- statistics
---
<p>
Let's suppose you see the following histogram of p-values: what do you
think is going on?
</p>


<div class="figure">
<p><img src="/img/conservative-p-values.png" alt="conservative-p-values.png" />
</p>
</div>

<p>
At first I thought there had been some mistake in the code where \(p =
1 - p\). But no, it's not a coding mistake.
</p>

<p>
Alternatively it could be an ill-advised one-sided test. However this
isn't the case since it's two-sided.
</p>

<p>
My final thought was that some assumption broke. It's always the last
thing you think of.
</p>

<p>
The test that I was running is the Kolmogorov-Smirnoff (KS) test for
goodness-of-fit. It's designed to test whether a sample came from a
particular distribution by looking at the maximum difference between
the CDF of the sample and the CDF of the distribution. Since I was
working with scaled (mean = 0, sd = 1) data I just compared with the
\(N(0,1)\). That's when I saw a figure much like Figure
<i>fig:conservative-p-values</i> which is actually generated from the null.
</p>

<p>
This is bad because p-values under the null should be uniform
otherwise we don't have the specified Type-I error. In this case we
have a conservative test so the actual Type-I error is less than than
the specified Type-I error. Usually people are more concerned when a
test is liberal (actual Type-I error greater than specified) since the
whole point of hypothesis testing is to control the Type-I error rate.
However, conservative tests are concerning as well since they leave
power on the table and make more Type-II errors than necessary.
</p>

<p>
I said before that an assumption broke. It turns out that what's
happening with the scaled is that the CDFs are shifted closer
together. The null distribution for the KS statistic doesn't take this
into account; it assumes your distribution was fixed and not fitted to
the data. Scaling the data implicitly fits the mean and standard
deviation of the normal and thus the assumption is broken.
</p>

<p>
To emphasize how fragile this assumption makes the KS test I draw
standard normal data and try the KS-test with the scaled data with the
raw data. This rescaling is essentially doing nothing to the data, but
the effects on the test are substantial. Figure <i>fig:ks-test-statistics</i>
show that the distribution of test statistics for the fitted
distribution (red) is pushed left which will result in low p-values
since the KS test assumes the raw distribution (blue) is the null.
</p>


<div class="figure">
<p><img src="/img/ks-test-statistics.png" alt="ks-test-statistics.png" />
</p>
</div>

<p>
With the constraint that you need to specify a single distribution the
KS test becomes less useful. Especially since I wasn't the one doing
the scaling. Fortunately there are workarounds.
</p>

<p>
For normal distributions there's a modification of the KS test which
will work for the fitted data: the Lillifors test. It uses the same
test statistics just with the correct distribution. I inadvertently
replicated the original paper's <sup><a id="fnr.1" name="fnr.1" class="footref" href="#fn.1">1</a></sup> method
for determining that distribution in Figure <i>fig:ks-test-statistics</i>;
it's just a Monte Carlo sample from the null. There's also an analytic
approximation <sup><a id="fnr.2" name="fnr.2" class="footref" href="#fn.2">2</a></sup> used by R's <a href="https://cran.r-project.org/web/packages/nortest/index.html"><code>nortest</code></a> and
Python's <a href="https://statsmodels.org"><code>statsmodels</code></a>.
</p>

<p>
This idea can be generalized to other location-scale families since
the effect of parameter estimation is the same as a linear
transformation to "canonical" form like the standard normal. There are
also analytic ways of calculating these null
distributions <sup><a id="fnr.3" name="fnr.3" class="footref" href="#fn.3">3</a></sup>.
</p>

<p>
Outside of these families the effect of parameter estimation could
depend on the unknown parameters which complicates things greatly.
Fortunately you can use the bootstrap in these cases <sup><a id="fnr.4" name="fnr.4" class="footref" href="#fn.4">4</a></sup>.
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">References</h2>
<div class="outline-text-2" id="text-1">
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" name="fn.1" class="footnum" href="#fnr.1">1</a></sup> <p class="footpara">
Lilliefors. (1967). <i>On the Kolmogorov-Smirnov test for normality with mean and variance unknown</i>. JASA. <a href="https://pdfs.semanticscholar.org/4aad/1756e88dba86399a75891895e00b160f5460.pdf">link</a>
</p></div>

<div class="footdef"><sup><a id="fn.2" name="fn.2" class="footnum" href="#fnr.2">2</a></sup> <p class="footpara">
Dallal and Wilkinson. (1986). <i>An analytic approximation to the distribution of Lilliefors's test statistic for normality</i>. Taylor &amp; Francis Group. <a href="http://amstat.tandfonline.com/doi/abs/10.1080/00031305.1986.10475419">link</a>
</p></div>

<div class="footdef"><sup><a id="fn.3" name="fn.3" class="footnum" href="#fnr.3">3</a></sup> <p class="footpara">
Durbin. (1973). <i>Distribution theory for tests based on the sample distribution function</i>. SIAM.
</p></div>

<div class="footdef"><sup><a id="fn.4" name="fn.4" class="footnum" href="#fnr.4">4</a></sup> <p class="footpara">
Babu and Rao. (2004). <i>Goodness-of-fit tests when parameters are estimated</i>. Sankhy: The Indian Journal of Statistics <a href="http://sankhya.isical.ac.in/search/66_1/2004005.pdf">link</a>
</p></div>


</div>
</div>
